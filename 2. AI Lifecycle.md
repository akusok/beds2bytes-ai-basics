# AI in Healthcare

## Course description

Welcome to the course **AI in Healthcare**! The aim of this course is to develop digital competence of using AI at work in general, and its possible applications to healthcare. At the end of this course you should be able to give an informed judgement on proposed AI-based systems at work, discuss new applications of AI with their benefits and drawbacks.

Learning outcomes:

* You understand modern AI landscape, what happens when someone sends a message to an AI-powered tool, and know major AI players.
* You know AI concepts like deployment, context, agents, tools, MCP protocol, and guardrais.
* You know about privacy and ethics regulations for AI in Europe
* You have some ideas about benefits that AI could bring to patients, and who are most likely to adopt it first

The course is organized in four parts starting with a short introduction video, followed by reading material and a quiz.

1. A brief introduction to AI
2. AI Lifecycle: What happens when I send a message?
3. Privacy, Ethics, and other creative ways to "shoot yourself in a foot"
4. AI, Internet and digital literacy in healthcare


## 2. AI Lifecycle: What happens when I send a message?

### AI Models and AI Businesses

Let's make a clear separation: What happens when I send a message? vs. How is it sold to consumers as a service?

Smallest example of asking an AI model: download weight files, run a program with weights and my question as input, see it using the GPU, read output. Then do the same in the AWS Bedrock cloud.

Discuss about a business model around it: How do you sell the output text? Cloud services, subscriptions, payment models, model switching etc.

Discuss training: a business wants to release new model each year (better weights file), and training a model needs data. Imaging "we are recording the calls for training purposes" but for everything you write to the model. Show new Ollama with account info and sharing, vs old Ollama with a software running locally.


### Current world players

Show a world map with the biggest players.
AI services:
    - USA: OpenAI ChatGPT, Anthropic Claude, Google Gemini, Meta LLaMA (small model)
    - Europe: Mistral from France, lots of smaller models
    - China: Huawei, Baidu, Tencent

Vision: OpenAI DALL-E, Stability AI - Stable Diffusion (open sourced), Midjourney (Canada)
Speech: OpenAI Whisper, a million smaller model variants, European multilingual models
Community: HuggingFace (GitHub of AI models, all kind of models)


### Where to run your models?

First let's look at the stuff that runs models.

Hardware:
    - NVidia "PROFESSIONAL" cards: consumer cards have no memory (max 24GB is a joke when models need 200+GB), professional cards are great but prices start at â‚¬30,000.
    - Apple Processors: CPU + a very strong GPU in one package + very fast and very large memory, ideal setup for smaller LLMs but expensive.
    - AMD Ryzen AI processor: a copy of Apple processor with CPU + strong GPU in one package, fast and large memory soldered to the computer (cannot change memory). Expensive.
    - Regular processors from Intel and AMD running LLMs with build-in GPU, can run small models slowly.
    - Gaming GPUs, memory capacity is a joke for LLMs but the small models that fit into a GPU can run very fast. Useful for image and video models that are smaller but need more computation.

Options for running LLMs:

    - Big & Expensive models: Big Corporations buy very expensive GPUs, train their own expensive models, and sell access to them (OpenAI, Anthropic etc.) Don't care much about data protection because they are in US/China and need to release new models regularly to stay in business.

    - Cheaper Big models available for "free" (they record your data): Google Gemini Fast online, Microsoft Copilot in Visual Studio Code app. Don't care about data protection at all because they need a way to pay for the models by selling your data.

    - Very small models running locally for free, but need a new and rather expensive computer for an OK user experience. Will get better and run on cheaper computers in the next 2-5 years. 
    - Very small models can run very fast on gaming GPUs, can serve multiple people from one desktop computer like on a website.
    Maximum data protection because everything runs on your computer by you.

**Saved by Capitalism!**
    - Cloud providers (AWS, Azure, Google Cloud) can run any model for you (even Big & Expensive) paying per usage. Can use a server located physically in EU and abiding European laws.
    Very good data protection because you pay them money directly, and they are willing to do anything a customer needs to continue paying them.


### TODO:

    - use cases: reduce toil; must access personal data but reduces interrupts or interactions on very simple and repetitive topics; keep human access easy because people may not share true concerns with AI
