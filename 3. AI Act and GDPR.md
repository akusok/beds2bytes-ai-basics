# AI in Healthcare

## Course description

Welcome to the course **AI in Healthcare**! The aim of this course is to develop digital competence of using AI at work in general, and its possible applications to healthcare. At the end of this course you should be able to give an informed judgement on proposed AI-based systems at work, discuss new applications of AI with their benefits and drawbacks.

Learning outcomes:

* You understand modern AI landscape, what happens when someone sends a message to an AI-powered tool, and know major AI players.
* You know AI concepts like deployment, context, agents, tools, MCP protocol, and guardrais.
* You know about privacy and ethics regulations for AI in Europe
* You have some ideas about benefits that AI could bring to patients, and who are most likely to adopt it first

The course is organized in four parts starting with a short introduction video, followed by reading material and a quiz.

1. A brief introduction to AI
2. AI Lifecycle: What happens when I send a message?
3. Privacy, Ethics, and other creative ways to "shoot yourself in a foot"
4. AI, Internet and digital literacy in healthcare



## 3. Privacy, Ethics, and other creative ways to "shoot yourself in a foot"

Good intentions are not compliance! Have the basics correct from the start.

New ways of engaging with patients, especially remote engagements based on IT technology, should have a well thought fundamentals that protect personal data but stay simple enough that the service remains convenient.

Think "Nordea bank interactions" with a separate authenticator app, that is also used to authenticate a user calling by phone that gives the bank worker access to that specific user's personal information and details necessary to answer questions or resolve issues.


In practice, introducing a new AI system in healthcare is like building a house. You cannot simply go out and start laying bricks. One must start from approved blueprints and permits. The structured and compliant process of building (or taking into use) new AI systems in Eupore is managed by two legal things: **GDPR** (General Data Protection Regulation) and **AI Act**.


### AI Act and GDPR

Two legal frameworks that govern modern IT systems in Europe, especially in impactful areas like healthcare.

* AI Act (newer) focuses on AI itself. It ensures that AI systems in Europe are safe and respect fundamental human right. AI Act classifies AI systems based on their potential risk for individuals, and for the society.

* GDPR (older) focuses on personal data processed by IT systems. It gives individual persons control over their personal data. In practise, GDPR regulates collecting, using, and storing personal data.


### GDPR High-level principles

Read GDPR overview: https://gdpr.eu/what-is-gdpr/ 

* Data collection must have a purpose | One cannot collect data "just in case"

* Patients own data about themselves. They can request editing or deleting it at any time. IT systems must enable such capabilities. (e.g. no personal data in system logs, or a service that cleans data out of system logs - no personal data is way easier)

* Security (logins, encryption, etc.) planned before creating a service. Slapping security as a patch on an existing services is a recipy for failure. Security plan includes breach/hack scenario, and limits the data affected by it, e.g. stealing a database is useless because all records are encrypted and different patients use different encryption keys.


### AI Act High-level principles

Read the AI Act 10-minute summary online: https://artificialintelligenceact.eu/high-level-summary/ 

* AI is a tool, it takes no responsibility! Diagnoses are made only by medical professionals, that can use AI to support their own decision making. (An excavator is not responsible for digging a foundation; it's operator is.)

* Some AI use cases are discouraged or even strictly prohibited. Always write a plan to validate your usage. Think of a university making a study on patients; AI is a new tech and its applications are similar to medical studies on real people.

* Always explicitly tell a person they interact with AI: "Hello, I am an AI chatbot. Can I help you?", "Now I will transfer you to a person to help with your question".

* Also AI-supported decisions should be transparent, and patients should have a way to challenge these decision.

* Take measures for unbiased processing, e.g. AI system trained on adult male date (convenient for that particular research) can gives wrong predicitons for women or children.


### It's dangerous to go alone. Build a team!

GDPR and other regulations look like a complicated mess the first time you deal with them. Find someone experienced who can help. This may be a legal or Data Protection person at your workplace, at another hospital, a teacher for the suitable topic. Investing time in understanding legal data protection requirements is the most valuable thing one can do at a begining of new IT project that involves patients. Misunderstandings or concerns that were forgotten at the planning stage can kill an otherwise very successful project liked by both medical practitioners and patients.

* Example data protection: https://thl.fi/en/about-us/data-protection

* Example guidelines for researchers: https://www.helsinki.fi/en/faculty-medicine/research/services-researchers/guidelines-data-protection 

* Data Protection Officer is your friend.

* Searching for information online, make sure to use European websites. US and UK have different laws, and their recommendations may be wrong for a European hospital.



### New AI System 1: Preparation

This is the planning step that one should do before writing a single line of code or considering a single provider of a new technology.


1. Define the intended purpose and classify your risk (AI Act)

Clearly define what you would like to do with the new system, and what it will not do. The answer to this question is fundamental in all following work. 

Example: if you build a house, decision on whether to build a living space or a commercial building results in very different building codes and requirements


2. Conduct the Data Protection Impact Assessment [DPIA](https://www.dataprotection.ie/en/organisations/know-your-obligations/data-protection-impact-assessments). It is legally required by GDPR for processing sensitive data at scale, which is basically any AI system.

You will map out how patient data will be collected / used / stored; identify potential risks like data breaches or incorrect AI decisions; and define measures to limit risks or impact of the risks (encryption, access control, audits)


3. Establish a lawful basis for data processing (GDPR)

You must have a valid legal reason for data processing, and a clear, unambiguous permission from patients. Also think if the data may be useful for research, and plan for possiblity of this use.

"Data processing" here refers to any use of any data involving patients. This is not limited to automated AI decisions or recommendations on medical diagnoses. Sending a checkup email is also "data processing" that involves a person's email address and a name to address them politely.

I know valuable research datasets (like a large spoken language dataset) collected long ago that are sadly impossible to use because the researchers back then did not collect a written agreement from people to use their data. However obvious this is, without legal grounds the data will be impossible to use. Take care in setting up legal grounds for data processing in advance.


### New AI System 2: Development

4. Implement data governance, track data usage & quality

From GDPR perspective, use robust data security measures with encryption and access control, and follow the data minimization principle by storing only the data that you actually need.

Form AI Act perspective, document the datasets used for training. Investigate training data on its correctness, representativeness, and any biases.

5. Keep the detailed logs on AI system architecture, algorithms, performance metrics, data used in training, and instructions for use. These documents may be required for inspection to prove compliance. (Modern experimentation systems will provide the tools for such logging)


### New AI System 3: Deployment

6. Do the Conformity Assessment (AI Act)

The compliance of high-risk AI systems must be formally assessed. This can be a self-assessment resulting in a written document, or an external audit. Assessment includes technical documentation, risk management system, and overall compliance.

AI Act Conformity Assessment for high-risk systems: https://artificialintelligenceact.eu/article/43/ 


7. Register the high-risk AI System and receive EU Declaration of Conformity (AI Act)

This is a document that allows your high-risk AI system to be used with actual patients/customers.

EU Declaration of Conformity for high-risk systems: https://artificialintelligenceact.eu/article/47/


### New AI System 4: Operation

8. Implement continous monitoring and human oversight (AI Act)

The system must provide a way to track its ongoing performance in the real world, and it must report serious data incidents or malfunctions, also to authorities. The system design must enable healthcare professionals to oversee its operations and intervene if necessary. This is a mandatory requirement.

Simple example: when HSL has app service slowdown, the app displays a clearly visible red notification about the issue to all users.

9. Continuously uphold data subject rights (GDPR)

Patients may request updating their data, fixing data mistakes, or even deleting personal data. These requests must be executed in a timely manner (~30 days). 

There are online services that promise people to delete any personal data stored about them on the Internet, so expect to have data deletion requests on a regular basis. Make sure there is an automated system fulfilling these requests, and monitor that it is actually operating.

